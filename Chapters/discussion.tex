\chapter{Discussion}\label{cha:discuss}

\section{Bug}\label{sec:discuss_bug}
In constraint (\ref{eq:116}) we calculate $k$ by taking the absolute value of the value for the tool used by the predecessor subtracted from the value of the tool used in this task. This was supposed to work with the 3-dimensional time matrix defined in the variable (\ref{eq:44}), but this is not always the case. The thought behind it was to through the method used for $k$ get an unique value for the transition between two tools. This works as intended for when $nbrTools \leq 2$, but when the number of tools increase it will in most cases not work as intended. Lets assume we have two tools, we have the option to make a tool change, since there is only one other tool available, or no tool change. Hence the following options can be calculated by the model; $abs(1-1)=0$, $abs(2-2)=0$, $abs(1-2)=1$ and $abs(2-1)=1$, which is correct or the 3-dimensional time matrix created for 2 tools. However, if we have 3 tools instead, the options as presented for 2 tools will be here as well, but also $abs(1-3)=2$, $abs(2-3)=1$, $abs(3-1)=2$, $abs(3-2)=1$, $abs(3-3)=0$. Here we can see that the change between tools 2 and 3 will yield the same $k$ value as the change between tools 1 and 2, which means this way of calculating the $k$ value as a unique value does not work for $nbrTools > 2$. There is one instance in which it would work for $nbrTools > 2$, and that is in the case of when all the tools are gathered in the same physical location, i.e.\ the move times to them are the same, and the time to change each tool is the same. However, if that is the case, the matrix is quite redundant since it would only need two change options, as for two tools, to change or not to change. Nevertheless, as we saw this works when we have 2 tools, which is the same amount of tool used in the case study and that means that the evaluation and results still holds up for the case study.

The approach of considering the move between two tools to take the same amount of time regardless of direction was chosen because it was concluded that it would be easier to model.

The fix to this bug would be to construct a table or 4-dimensional matrix that could hold all the possible values for the changes. It could also be created so that there could be different times depending on if the change is from tool 1 to tool 2 or from tool 2 to tool 1.

\section{Model}
As we mentioned earlier, the design of this model has taken inspiration from the model in \cite{ejenstam_2014}. The development started out with the goal to solve a similar problem but in a different way. Ejenstam solves it using Vehicle Routing Problem (VRP), but we wanted to try and solve it as a Job Shop Problem. We read through his thesis and took some inspiration from the way of tagging or categorising tasks, such as using tray, using fixture, etc., while trying to not be too much influenced by his solution. But as the development moved on we found our solution tending more and more towards Ejenstam's. For example, we wanted our model to be as pure of a job shop problem as possible, but then we needed to know what task came before a certain task and that warranted for predecessors and creating circuits. Although, there are some differences in our case studies that affect our models in different ways.

In Ejenstam's model each sub-assembly has its own dedicated fixture. Or rather, each sub-assembly that does not require any lifting from the fixture. Where the limit goes is not clear. Take for example the case of putting \emph{component1} in a fixture, mounting \emph{component2} on \emph{component1}, pick upp sub-assembly \emph{component1-component2}, do some operation, maybe turning it over, putting it back in a fixture, and mount \emph{component3} on sub-assembly \emph{component1-component2}. Do we need two fixtures or does one suffice? In our model we can cope with only one fixture even if we have more than one sub-assembly, as in our case study. But this also means we need to ensure that no part is put in each fixture before the fixture has been emptied. We do this with constraint (\ref{eq:100}) where we identify which put and take is associated with each sub-assembly on the fixtures. One does not need to do that in Ejenstams model since there are as many fixtures as sub-assemblies and therefore automatically only one put and one take task for a single fixture. It makes Ejenstams model simpler in this aspect, since the problem of assigning which sub-assembly gets which fixture and when it gets it is solved before the solver tries to solve the problem. In our model it is part of the problem for the solver to solve.

In both Ejenstam's model and our model we try to perform collision avoidance. By that we mean that we avoid collisions in, for example, fixture, it does not mean that we avoid collisions altogether. We do not assure that arms do not collide in mid air. Although, it is somewhat accounted for when we say that arms cannot reach certain tasks. Since the robot has two arms positioned in such a way that we could divide the work area in two parts where one arm is responsible for one part and the other arm for the other part. In this way we could avoid them colliding when moving about. Although, this might not be the best strategy when aiming for the smallest makespan. The feature of being able to assign tasks that certain arms cannot reach was developed last in our model and right before implementing that feature we got the makespan of 504 time units. Compared to the result we got after that feature we would get a better makespan if the feature was not used. This is hinting that it could be beneficial to not constrain the tasks the arms cannot reach too much.

Another thing that needs to be considered to avoid the machines from colliding is where they go when a task is completed. Because there can be a wait between when a task ends and the next task starts the machines can be stationary at some point in space. This is nothing our model takes into consideration, instead our model assumes that the machine performing the task does not remain at the point where the task was performed after the task ends, as can be seen in constraints such as (\ref{eq:104}). Instead it is something the one who creates the data to be scheduled needs to think about. The one creating the data might create tasks in such a way that they actually represent the task and a move away from the task to a spot where the machine will be able to wait for the next task. This will not eliminate the problem of the other machine colliding with the machine when it waits in the new spot. But it will eliminate the problem of machines colliding at, for example, a fixture when one arm has put a component in the fixture and the other machine wants to mount another component.

Instead of being able to change tool as in our model, Ejenstam's model gives the possibility to have many tools on the same arm at the same time. Our ambition was to be able to solve the data for Ejenstam's case study using our model. And thanks to Johan Wessen at ABB we were allowed access to the data used. Unfortunately, this part of the functionality of Ejenstam's problem is what put a spanner in the works for that ambition. Because if we wanted to solve Ejenstam's data we would need to be able to have hands with many tools. As it is now we can have one tool mounted on a machine at a time with the possibility to switch to another tool. If we have a case were we have \emph{tool1} mounted on the machine but we need \emph{tool2} there is only one thing we can do, and that is to change the tool. If we incorporated the possibility to have many tools on one hand and have the case of having three hands were \emph{hand1} having one \emph{tool1}, \emph{hand2} and two \emph{tool2} and \emph{hand3} having one \emph{tool2} and one \emph{tool3}, having \emph{hand1} mounted on the machine and needing \emph{tool2}. This gives the possibility to either switch to \emph{hand2} or \emph{hand3} and thereby increasing the complexity of the problem because a decision on what hand to mount on the machine at this time will affect the need to change hands in the future in a greater way compared to changing single tools as we do in our model.

By only allowing one tool mounted at a time in our model we can filter the precedences of certain types of tasks. For example in our model, because of only having one tool at a time, we can only pick up one component at a time and we have to put it down or mount it before picking up another tool. It is in contrast to Ejenstam's model where we can pick up several components after one another, the amount depending on the hand mounted on the machine. Because of this, we can in our model filter taking tasks so they cannot be predecessors to one another and thereby reducing the search space. We do this filtering with constraints (\ref{eq:75}) and (\ref{eq:76}).

In a way our model is more complex than Ejenstam's because we have the options to customise mountings and movements, such as mounting in mid air using only the two machines and no fixture. Or use fixtures to store components or sub-assemblies. But Ejenstam's search space is much larger, mainly because of two reasons. One, they want to not only find an optimal solution using one fixed setup of trays, but rather how can we place the trays to get the most optimal makespan. Which is like our case study, but with another layer on top that needs to be solved. And two, because they can pick up several components after one another before mounting or putting them down. These two factors makes Ejenstam's search space much larger than ours and thus it does warrant for using customised search strategies as the ones tested in his thesis. Since out case study is relatively small it might not necessarily need some customised searches. But as our results show, this seem to vary depending on what solver is used. And also, if the assembly would be larger, i.e. more tasks to perform, such as if we would like to schedule several cycles of the assembly, there might come a need for using customised searches such as the ones in Ejenstam's thesis.

As opposed to our case study, Ejenstam made a time study where he measured the times both for the time of the moves between tasks and for the time of the tasks themselves. The move times where measured using RobotStudio \cite{robotstudio}. Whether RobotStudio was used for measuring the task times is unclear. While we on the other hand estimated our times using a video. Ejenstam used the exact measured times from the time study and by looking at the data from the study one can see that the times vary somewhat on the same task depending on what machine performed the task. This might seem a bit odd, but it could be due to an unknown factor that might have to be considered when a task is performed. It could also be a coincidence when the measurements were made. If this is the case, measurements that is prone to errors that will make the times between arms on the same task different from one another have the potential to introduce unfair advantages to one of the machines. In an ideal environment a task should take the same amount of time to perform independently of the machine performing it. This is how we look at it, we use the same times for both arms. Although, as said before, we use estimated times, so our times are even more error prone than Ejenstam's. But our times does not have the potential to give unfair advantages to one machine or another. But it is a big source of error when it comes to comparing the time from the solver and the real assembly in the video. Since we first estimated the times and then modified them a bit, we could not simply compare the results from the solvers with the time in the video. Rather, we had to analyse the video again and try to append the estimated times on the tasks in the video and when they occurred and get a total time of the assembly that way. In future works it would be beneficial if a time study was performed for the use case.
\\\\
In our model we represent tasks that need multiple machines as many tasks as there are machines needed together with a demand that they need to be executed simultaneously. Another possible way to model it would be to have one task with many machines. But that would mean that tasks could have many predecessors since there are as many tasks previous to this tasks as there are machines performing this task that need to be linked with this task. And it would also require all the precedence constraints to be remodeled and the \texttt{circuit} constraint would not work for this model anymore since tasks would now be able to appear in multiple places on the circuit. So the approach of using multiple tasks to model a task using multiple machines might need additional information from the input, but in all is easier to model with constraints.

In our input to the model we provide the time in a matrix form with all the possible needed times already calculated. This seems to give the solver a good performance since it does not need to calculate it on the fly over and over again. We do a similar thing with the time matrix file we use to generate the input file to the solvers, see \ref{sec:time_matrix}. Since there are many tasks performed in the same physical location, as in our case study where all fixture related tasks are performed in the same fixture, a change in the location of the fixture would make it a hassle to change all the affected times. We could solve it in the XML file, see \ref{sec:xml}, by associating all the locations used with keys and then each task would be related to one of those keys. Then the translation tool, see \ref{sec:assemblyConv}, could look up the position for the tasks and calculate the time the time for the movement between them.

There is a filter applied to the $moveDuration$ for each task that limits its domains, the (\ref{eq:67}) constraint. This filter might not be necessary since the values for $moveDuration$ are assigned using the $3DTimeMatrix$ and therefore not searched like a regular domain. So limiting the domain to just the values it can be assigned from the matrix probably does not make any difference.

When running a smaller test with an assembly that did not have neither concurrent tasks nor ordered tasks it was discovered that assemblies such as that one could not be run using version 2.0.1. The error occurs when translating the MiniZinc code into FlatZinc, so it seems that 2.0.1 handles empty arrays in a different way and does accept them.

\section{Results}
The time from the handmade assembly was 514 time units and the one done by the solvers was 512 time units. The ordering of the tasks is identical in the two solutions. By studying the handmade assembly and the one from the solvers, one can see that the difference lies in that in the handmade the tasks sometimes wait for the another task to finish before starting, or even starting to move to the task. In the assembly created by the solvers, tasks sometimes had to do the same and wait for another task to finish. But that depended on what relationship there was between the tasks, and sometimes the task could proceed earlier than in the handmade assembly and thereby shorten the makespan.

The intention when we started testing was to generated the FlatZinc files and the let the solvers run on them in order to remove the time it takes to translate the MiniZinc file from the measurements. However, one of the solvers took longer time to solve the problem if it just ran the FlatZinc file directly instead of running the default command that translates and runs the solver directly. It seems odd, but was something we just had to deal with. Because of this, all the tests were ran with the default commands that first translate the MiniZinc file and then run the solver. This affects the times, although differently depending on if the solve time relates to the parse and translate time of the MiniZinc file. Although, when one runs the solvers to solve a problem, one will probably use the command to first translate the MiniZinc file and the run the solver, so doing the measurement for that case comes closer to the real use case than not incorporate the compilation.

As mentioned before. Opturion CPX can in some instances be a faster solver than a regular FD solver. However, as can be seen in the table \ref{tab:res_cpx} this does not seem to be the case for this set of data and model.

As can be seen in table \ref{tab:res_cpx} Opturion CPX produces files with much more variables and constraints than the other solvers. This is probably because it is a solver that combines SAT and FD to solve the problem and thus it would probably need to translate many relations into pure boolean relationships, making the files larger.

The two fastest solvers, Gecode and or-tools, were amongst the solvers were the number of cores could be specified. And as said, they were given access to all 4 cores when running, giving them a slight upper hand compared to the other solvers. But, as we can see from the results, one of the solvers which could not solve the model at all was also among the solvers that got access to all the cores, namely Choco3. Therefore, there seems to be a small connection between the number of cores and the time it takes to solve the problem, but it is not necessarily true for all solvers.

By looking at the results of JaCoP, Gecode and or-tools we can see that the filters do make a difference in the runtime. Comparing using both groups of filters versus using no filter it shows that the filters can have quite a large impact on the runtime. However, concluding which of the filter groups works the best is hard. If we look at Gecode and or-tools, they seem to show that the temporal filter makes a little more improvement than the predecessor filter. Looking at only the result from Gecode, the temporal filter makes a much greater improvement than the predecessor filter. The predecessor filter is even a little bit slower here. However, if we look at the result for JaCoP, it is the other way around. Here the predecessor filter makes a much greater improvement, it cuts down the time from 4+ hours to just 16 minutes.

As mentioned, when using the 1.6 version the filters increase the number of constraints quite a bit, but the number of reified constraints are decreased at the same time by quite a lot as well. This seems to indicate that the filters do not introduce more reified constraints, or at least not a significant amount, which is good.

The goal of a filter is to decrease the domains of the variables, so the optimal solution would be to just remove the values from the domain without adding new constraints. And we can see when using version 1.6 that both the temporal filter and the predecessor filter introduce new constraints. We briefly analysed how JaCoP handled these constraints and we saw that it used those constraints to prune the domains and removed the constraints. This means that in 1.6 MiniZinc hands over the pruning to the solvers, but there is no guarantee that the solvers will prune the domains. Looking at the results for when applying the filters using the 2.0.1 version we can see that the number of constraints is not affected, or in the predecessor case affected very little. This seems to indicate that the translator does the pruning itself instead of handing it over to the solvers.



