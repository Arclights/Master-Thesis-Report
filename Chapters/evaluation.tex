\chapter{Evaluation}\label{cha:eval}
Here we will present the evaluation of the model. Each model was given a time limit of 4 hours to complete as it seemed long enough to guarantee us a result from all the solver, but short enough to be manageable to test. But as can be seen in section \ref{sec:res}, this was not the case. In the case where the solvers could produce a result in the given time frame the solvers where run 10 times and the average time was used.

Originally, the 1.6 version of MiniZinc was used. But during the making of the model version 2.0 was released. According to NICTA, the 2.0 version is a complete rewrite of the MiniZinc-to-FlatZinc comipiler but that the resulting FlatZinc file is compatile with 1.6 solvers, and therefore no changes should be required for the solvers.\cite{mz2} We therefore thought it could be of interest to compare how the 2.0 version performed compare to the 1.6 version. During the test phase of the thesis version 2.0.1 was released that should fix some bugs in 2.0.\cite{mz2_changelog} Therefore, version 2.0.1 is used.

Apart from measuring the the time, we also analysed the resulting FlatZinc file to see if we could see any correlation between the running time and the data run. The data extracted was the number of integer and boolean variables, the number of arrays, the number of constraints and the percentage of constraints that where reified. We measure the reifieds since even if we try to avoid direct reifieds in the MiniZinc code, the FlatZinc could still contain reified depending on how it translates the MiniZinc code.

In order to see if the filtering we introduced made any difference, we also measure the different combinations of the filters and the absence of filters.

Because of all the combinations of parameters we want to test, there will be 8 test cases for each solver.

What we want to achieve is to get the solver to reach the optimal solution, that is what we mean when we say result.

\section{The Setup}
The computer used to run the solvers was equipped with an Intel i7 2.8GHz quad core CPU, 8GB DDR3 1333MHz memory and running Mageia 4.
\\\\
The versions of each of the solvers are presented in the table below.
\begin{table}[h]
\begin{tabular}{l|l}
Solver                    & Version \\ \hline
G12/FD                    & 1.6.0\\
JaCoP                     & 4.2 \\
Gecode                    & 4.3.2 \\
or-tools                  & Rev. 3782 \\
Opturion CPX              & 1.0.2 \\
\multirow{2}{*}{Chcoco 3} & Solver: 3.2.2 \\
                          & Parser: 3.2.0
\end{tabular}
\end{table}
Note that for Choco3, the solver and parser has different version number. This is because they are not distributed together and therefore have slightly different version numbers.

The versions of the solvers used are the latest as of December 2014.

As mentioned before, the versions of MiniZinc tested are $1.6$ and $2.0.1$.

\section{The results}\label{sec:res}
The results presented in the tables \ref{tab:res_g12} to \ref{tab:res_choco} are the analysis of the FlatZinc files. The tests are grouped into 4 categories of runs; with predecessor filter and domain filter, with only predecessor filter, with only domain filter and with no filter at all. In each of these groups there are two groups, one for each version we tests, i.e. $1.6$ and $2.0.1$

In the tables presented the filter names has been shortened to \emph{pred} and \emph{dom}. \emph{pred} means the predecessor filter and \emph{dom} means the domain filter. 


\input{Tables/results_g12}

\input{Tables/results_jacop}

\input{Tables/results_cpx}

\input{Tables/results_gecode}

\input{Tables/results_or}

\input{Tables/results_choco}

If one just do a quick comparison of the numbers in the tables without doing any deeper analysis, one can see that the results from Gecode, or-tools and Choco3, shown in tables \ref{tab:res_gecode}, \ref{tab:res_or} and \ref{tab:res_choco}, respectively, are identical. This hints that the FlatZinc files for these solvers should be very similar. And indeed, if one compares the FlatZinc files for these three solvers for one of the cases, they are as good as identical. The difference between them is the naming of some constraints and sometimes the placement in the file.

We will present some more tables that show the change in the values. The first 4 columns are grouped by the version of MiniZinc and in each of those groups we show the change in the values if each of the filters are added. The last column shows the change in values if going from using the $1.6$ version to using the $2.0.1$ version. The values presented are averages. For example, the value for when adding the domain filter for version $1.6$ is the average of the changes from not using any filters to using the domain filter and from using the predecessor filter to using the domain and predecessor filters. The average for the last column is just the average of the changes for all cases when switching from $1.6$ to $2.0.1$

\input{Tables/results_diff_g12}

\input{Tables/results_diff_jacop}

\input{Tables/results_diff_cpx}

\input{Tables/results_diff_gecode_or_choco}

For G12/FD we could not get any results for any of the tests despite using filter. Comparing 1.6 and 2.0.1 shows that there is an increase in variables and a decrease in constraints for the 2.0.1 version. Also interesting to note is that in version 2.0.1, the domain constraints does not introduce any new constraints. This should be expected for filters. This is not the case for 1.6. The filters does not seem introduce any new reified constraints as the percentage of reified constraints decrease as more filters are introduced.
